{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twint\n",
      "  Using cached twint-2.1.20-py3-none-any.whl\n",
      "Requirement already satisfied: aiohttp in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from twint) (3.9.1)\n",
      "Collecting aiodns (from twint)\n",
      "  Using cached aiodns-3.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\gabri\\appdata\\roaming\\python\\python311\\site-packages (from twint) (4.12.2)\n",
      "Collecting cchardet (from twint)\n",
      "  Using cached cchardet-2.1.7.tar.gz (653 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting elasticsearch (from twint)\n",
      "  Using cached elasticsearch-8.11.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pysocks in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from twint) (1.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from twint) (2.0.1)\n",
      "Collecting aiohttp-socks (from twint)\n",
      "  Using cached aiohttp_socks-0.8.4-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting schedule (from twint)\n",
      "  Using cached schedule-1.2.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting geopy (from twint)\n",
      "  Using cached geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fake-useragent (from twint)\n",
      "  Using cached fake_useragent-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting googletransx (from twint)\n",
      "  Using cached googletransx-2.4.2-py3-none-any.whl\n",
      "Collecting pycares>=4.0.0 (from aiodns->twint)\n",
      "  Using cached pycares-4.4.0-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gabri\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->twint) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->twint) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->twint) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->twint) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->twint) (1.3.1)\n",
      "Collecting python-socks<3.0.0,>=2.4.3 (from python-socks[asyncio]<3.0.0,>=2.4.3->aiohttp-socks->twint)\n",
      "  Using cached python_socks-2.4.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gabri\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4->twint) (2.5)\n",
      "Collecting elastic-transport<9,>=8 (from elasticsearch->twint)\n",
      "  Using cached elastic_transport-8.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting geographiclib<3,>=1.52 (from geopy->twint)\n",
      "  Using cached geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from googletransx->twint) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gabri\\appdata\\roaming\\python\\python311\\site-packages (from pandas->twint) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->twint) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->twint) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->twint) (1.24.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch->twint) (2.0.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch->twint) (2023.7.22)\n",
      "Requirement already satisfied: cffi>=1.5.0 in c:\\users\\gabri\\appdata\\roaming\\python\\python311\\site-packages (from pycares>=4.0.0->aiodns->twint) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gabri\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->twint) (1.16.0)\n",
      "Collecting async-timeout>=3.0.1 (from python-socks[asyncio]<3.0.0,>=2.4.3->aiohttp-socks->twint)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp->twint) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->googletransx->twint) (3.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\gabri\\appdata\\roaming\\python\\python311\\site-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint) (2.21)\n",
      "Using cached aiodns-3.1.1-py3-none-any.whl (5.4 kB)\n",
      "Using cached aiohttp_socks-0.8.4-py3-none-any.whl (9.6 kB)\n",
      "Using cached elasticsearch-8.11.0-py3-none-any.whl (412 kB)\n",
      "Using cached fake_useragent-1.4.0-py3-none-any.whl (15 kB)\n",
      "Using cached geopy-2.4.1-py3-none-any.whl (125 kB)\n",
      "Using cached schedule-1.2.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached elastic_transport-8.10.0-py3-none-any.whl (59 kB)\n",
      "Using cached pycares-4.4.0-cp311-cp311-win_amd64.whl (76 kB)\n",
      "Using cached python_socks-2.4.3-py3-none-any.whl (52 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Building wheels for collected packages: cchardet\n",
      "  Building wheel for cchardet (setup.py): started\n",
      "  Building wheel for cchardet (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for cchardet\n",
      "Failed to build cchardet\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [11 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\cchardet\n",
      "      copying src\\cchardet\\version.py -> build\\lib.win-amd64-cpython-311\\cchardet\n",
      "      copying src\\cchardet\\__init__.py -> build\\lib.win-amd64-cpython-311\\cchardet\n",
      "      running build_ext\n",
      "      building 'cchardet._cchardet' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for cchardet\n",
      "ERROR: Could not build wheels for cchardet, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gabri\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp) (3.4)\n"
     ]
    }
   ],
   "source": [
    "pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/twintproject/twint.git\n",
      "  Cloning https://github.com/twintproject/twint.git to c:\\users\\gabri\\appdata\\local\\temp\\pip-req-build-39mbvwwh\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 2] The system cannot find the file specified while executing command git version\n",
      "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/twintproject/twint.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3904147434.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[39], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    git clone --depth=1 https://github.com/twintproject/twint.git\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git clone --depth=1 https://github.com/twintproject/twint.git\n",
    "cd twint\n",
    "pip3 install . -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twint'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mGabrielSpagnuolo15\\schulich_data_science\\ACP_Twitter_Twint.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell://github/GabrielSpagnuolo15/schulich_data_science/ACP_Twitter_Twint.ipynb#W0sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtwint\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'twint'"
     ]
    }
   ],
   "source": [
    "import twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure\n",
    "c = twint.Config()\n",
    "c.Username = \"realDonaldTrump\"\n",
    "c.Search = \"great\"\n",
    "\n",
    "# Run\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snscrape\n",
      "  Downloading snscrape-0.7.0.20230622-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from snscrape) (2.31.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from snscrape) (4.9.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\gabri\\appdata\\roaming\\python\\python311\\site-packages (from snscrape) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from snscrape) (3.13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gabri\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4->snscrape) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->snscrape) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->snscrape) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->snscrape) (2023.7.22)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->snscrape)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
      "   ---------------------------------------- 0.0/74.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 74.8/74.8 kB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: PySocks, snscrape\n",
      "Successfully installed PySocks-1.7.1 snscrape-0.7.0.20230622\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import snscrape.modules.twitter as snt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23CIBC%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404)\n",
      "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23CIBC%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.\n",
      "Errors: blocked (404), blocked (404), blocked (404), blocked (404)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23CIBC%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mGabrielSpagnuolo15\\schulich_data_science\\ACP_Twitter_Twint.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell://github/GabrielSpagnuolo15/schulich_data_science/ACP_Twitter_Twint.ipynb#X15sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m scrape \u001b[39m=\u001b[39m snt\u001b[39m.\u001b[39mTwitterSearchScraper(\u001b[39m\"\u001b[39m\u001b[39m#CIBC\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell://github/GabrielSpagnuolo15/schulich_data_science/ACP_Twitter_Twint.ipynb#X15sdnNjb2RlLXZmcw%3D%3D?line=2'>3</a>\u001b[0m tweets \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell://github/GabrielSpagnuolo15/schulich_data_science/ACP_Twitter_Twint.ipynb#X15sdnNjb2RlLXZmcw%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, tweet \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(scrape\u001b[39m.\u001b[39mget_items()):\n\u001b[0;32m      <a href='vscode-notebook-cell://github/GabrielSpagnuolo15/schulich_data_science/ACP_Twitter_Twint.ipynb#X15sdnNjb2RlLXZmcw%3D%3D?line=5'>6</a>\u001b[0m     data \u001b[39m=\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell://github/GabrielSpagnuolo15/schulich_data_science/ACP_Twitter_Twint.ipynb#X15sdnNjb2RlLXZmcw%3D%3D?line=6'>7</a>\u001b[0m         tweet\u001b[39m.\u001b[39mdate,\n\u001b[0;32m      <a href='vscode-notebook-cell://github/GabrielSpagnuolo15/schulich_data_science/ACP_Twitter_Twint.ipynb#X15sdnNjb2RlLXZmcw%3D%3D?line=7'>8</a>\u001b[0m         tweet\u001b[39m.\u001b[39mlikeCount,\n\u001b[0;32m      <a href='vscode-notebook-cell://github/GabrielSpagnuolo15/schulich_data_science/ACP_Twitter_Twint.ipynb#X15sdnNjb2RlLXZmcw%3D%3D?line=8'>9</a>\u001b[0m         tweet\u001b[39m.\u001b[39murl\n\u001b[0;32m     <a href='vscode-notebook-cell://github/GabrielSpagnuolo15/schulich_data_science/ACP_Twitter_Twint.ipynb#X15sdnNjb2RlLXZmcw%3D%3D?line=9'>10</a>\u001b[0m     ]\n\u001b[0;32m     <a href='vscode-notebook-cell://github/GabrielSpagnuolo15/schulich_data_science/ACP_Twitter_Twint.ipynb#X15sdnNjb2RlLXZmcw%3D%3D?line=10'>11</a>\u001b[0m     tweets\u001b[39m.\u001b[39mappend(data)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\snscrape\\modules\\twitter.py:1763\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1760\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mvariables\u001b[39m\u001b[39m'\u001b[39m: variables, \u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m: features}\n\u001b[0;32m   1761\u001b[0m paginationParams \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mvariables\u001b[39m\u001b[39m'\u001b[39m: paginationVariables, \u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m: features}\n\u001b[1;32m-> 1763\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_api_data(\u001b[39m'\u001b[39m\u001b[39mhttps://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline\u001b[39m\u001b[39m'\u001b[39m, _TwitterAPIType\u001b[39m.\u001b[39mGRAPHQL, params, paginationParams, cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cursor, instructionsPath \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msearch_by_raw_query\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msearch_timeline\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtimeline\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minstructions\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m   1764\u001b[0m \t\u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graphql_timeline_instructions_to_tweets(obj[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msearch_by_raw_query\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msearch_timeline\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtimeline\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39minstructions\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\snscrape\\modules\\twitter.py:915\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m \t_logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRetrieving scroll page \u001b[39m\u001b[39m{\u001b[39;00mcursor\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 915\u001b[0m \tobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_api_data(endpoint, apiType, reqParams, instructionsPath \u001b[39m=\u001b[39;49m instructionsPath)\n\u001b[0;32m    916\u001b[0m \t\u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    918\u001b[0m \t\u001b[39m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\snscrape\\modules\\twitter.py:886\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[1;34m(self, endpoint, apiType, params, instructionsPath)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[39mif\u001b[39;00m apiType \u001b[39mis\u001b[39;00m _TwitterAPIType\u001b[39m.\u001b[39mGRAPHQL:\n\u001b[0;32m    885\u001b[0m \tparams \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39murlencode({k: json\u001b[39m.\u001b[39mdumps(v, separators \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mitems()}, quote_via \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39mquote)\n\u001b[1;32m--> 886\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(endpoint, params \u001b[39m=\u001b[39;49m params, headers \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apiHeaders, responseOkCallback \u001b[39m=\u001b[39;49m functools\u001b[39m.\u001b[39;49mpartial(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_api_response, apiType \u001b[39m=\u001b[39;49m apiType, instructionsPath \u001b[39m=\u001b[39;49m instructionsPath))\n\u001b[0;32m    887\u001b[0m \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39m_snscrapeObj\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\snscrape\\base.py:275\u001b[0m, in \u001b[0;36mScraper._get\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 275\u001b[0m \t\u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\u001b[39m'\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\snscrape\\base.py:271\u001b[0m, in \u001b[0;36mScraper._request\u001b[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[0;32m    269\u001b[0m \t_logger\u001b[39m.\u001b[39mfatal(msg)\n\u001b[0;32m    270\u001b[0m \t_logger\u001b[39m.\u001b[39mfatal(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mErrors: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(errors)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 271\u001b[0m \t\u001b[39mraise\u001b[39;00m ScraperException(msg)\n\u001b[0;32m    272\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mReached unreachable code\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mScraperException\u001b[0m: 4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23CIBC%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up."
     ]
    }
   ],
   "source": [
    "scrape = snt.TwitterSearchScraper(\"#CIBC\")\n",
    "\n",
    "tweets = []\n",
    "\n",
    "for i, tweet in enumerate(scrape.get_items()):\n",
    "    data = [\n",
    "        tweet.date,\n",
    "        tweet.likeCount,\n",
    "        tweet.url\n",
    "    ]\n",
    "    tweets.append(data)\n",
    "    if i > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets, columns=['Date', 'Like_Count', 'URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
